#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  v5.2-lite
- å¤šé¢æ€§ã‚’ç¶­æŒã—ãŸã¾ã¾å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ï¼ˆ2-3åˆ†ç›®æ¨™ï¼‰
- RSSãƒ•ã‚£ãƒ¼ãƒ‰æ•°å‰Šæ¸›ï¼ˆ74ä»¶ â†’ 30ä»¶ï¼‰
- å‡¦ç†ä¸Šé™è¨­å®šï¼ˆURLè§£æ±º200ä»¶ã€LLMåˆ¤å®š60ä»¶ï¼‰
- ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ï¼ˆmax_workers=10ï¼‰
- ãƒ­ã‚°å³æ™‚è¡¨ç¤ºï¼ˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–ï¼‰
"""

VERSION = "v5.2-lite-v3-frozen-20260115-0230"

import os
import feedparser
import requests
from delayed_valuable_news import is_delayed_valuable_news
from openai import OpenAI
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail
import json
import hashlib
import re
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import pytz
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from news_clustering_v51 import cluster_news_by_topic, prepare_delivery_news, print_clustering_log

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI()

# å°æ¹¾æ™‚é–“
TW_TZ = pytz.timezone('Asia/Taipei')

# çµ±è¨ˆæƒ…å ±
STATS = {
    'cache_hit': 0,
    'cache_miss': 0,
    'redirect_timeout': 0,
    'redirect_failed': 0,
    'sns_domain_excluded': 0,
    'sns_publisher_excluded': 0,
    'duplicate_excluded': 0,
    'unknown_publisher_excluded': 0
}

# éŠ˜æŸ„æƒ…å ±ã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
def load_stocks():
    """stocks.jsonã‹ã‚‰éŠ˜æŸ„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open('/home/ubuntu/stocks.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('stocks', {})
    except FileNotFoundError:
        print("ã‚¨ãƒ©ãƒ¼: stocks.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {}
    except json.JSONDecodeError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: stocks.json ã®å½¢å¼ãŒä¸æ­£ã§ã™: {e}", flush=True)
        return {}

STOCKS = load_stocks()

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆv5.2-lite: 30ä»¶ã«å‰Šæ¸›ã€å¤šé¢æ€§ç¶­æŒï¼‰
RSS_FEEDS = [
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘  éŠ˜æŸ„ç›´çµã‚¯ã‚¨ãƒªï¼ˆ10ä»¶ï¼‰
    # ========================================
    
    # å°ç©é›»ï¼ˆ2330ï¼‰ - 3ä»¶
    "https://news.google.com/rss/search?q=å°ç©é›»+OR+TSMC&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=TSMC&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=TSMC&hl=ja&gl=JP&ceid=JP:ja",
    
    # å‰µè¦‹ï¼ˆ2451ï¼‰ - 2ä»¶
    "https://news.google.com/rss/search?q=å‰µè¦‹+OR+Transcend&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å‰µè¦‹+OR+Transcend&hl=ja&gl=JP&ceid=JP:ja",
    
    # å®‡ç»ï¼ˆ8271ï¼‰ - 2ä»¶
    "https://news.google.com/rss/search?q=å®‡ç»+OR+Apacer&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Apacer&hl=en-US&gl=US&ceid=US:en",
    
    # å»£é”ï¼ˆ2382ï¼‰ - 3ä»¶
    "https://news.google.com/rss/search?q=å»£é”+OR+Quanta&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Quanta+Computer&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=å»£é”+OR+Quanta&hl=ja&gl=JP&ceid=JP:ja",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘¡ ä¸Šæµãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚¯ã‚¨ãƒªï¼ˆ13ä»¶ï¼‰
    # ========================================
    
    # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ6ä»¶ï¼‰
    "https://news.google.com/rss/search?q=EUV&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=CoWoS&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=HBM&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=æ¶²å†·&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å…ˆé€²è£½ç¨‹&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å…ˆé€²å°è£&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # é¡§å®¢ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆ3ä»¶ï¼‰
    "https://news.google.com/rss/search?q=NVIDIA&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=AIä¼ºæœå™¨&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=GB200&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # æ”¿ç­–ãƒ»åœ°æ”¿å­¦ï¼ˆ2ä»¶ï¼‰
    "https://news.google.com/rss/search?q=ç¾åœ‹å» &hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=é—œç¨…&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # éœ€çµ¦ãƒ»ä¾›çµ¦åˆ¶ç´„ï¼ˆ2ä»¶ï¼‰
    "https://news.google.com/rss/search?q=DRAMåƒ¹æ ¼&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=ç”¢èƒ½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘¢ æ¥­ç¸¾ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã‚¯ã‚¨ãƒªï¼ˆ4ä»¶ï¼‰
    # ========================================
    
    "https://news.google.com/rss/search?q=å°ç©é›»+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å‰µè¦‹+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å®‡ç»+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å»£é”+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘£ å…±é€šæ¥­ç•Œã‚¯ã‚¨ãƒªï¼ˆ3ä»¶ï¼‰
    # ========================================
    
    "https://news.google.com/rss/search?q=åŠå°é«”&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=DRAM+OR+NAND&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=ODM&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

# SNSãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆ
SNS_DOMAINS = [
    'threads.net',
    'instagram.com',
    'line.me',
    'linkedin.com',
    'tiktok.com',
    'youtube.com', 'youtu.be'
]

def is_sns_domain(url):
    """URLãŒSNSãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    url_lower = url.lower()
    return any(sns in url_lower for sns in SNS_DOMAINS)

def clean_url(url):
    """URLã‹ã‚‰ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
    parsed = urlparse(url)
    query_params = parse_qs(parsed.query)
    
    # é™¤å¤–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    exclude_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
                      'fbclid', 'gclid', 'msclkid', 'oc', '_ga', '_gl']
    
    # ã‚¯ãƒªãƒ¼ãƒ³ãªã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½œæˆ
    clean_params = {k: v for k, v in query_params.items() if k not in exclude_params}
    clean_query = urlencode(clean_params, doseq=True)
    
    # URLã‚’å†æ§‹ç¯‰
    clean_parsed = parsed._replace(query=clean_query)
    return urlunparse(clean_parsed)

def resolve_final_url(url, timeout=2):
    """
    ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’è¿½è·¡ã—ã¦æœ€çµ‚åˆ°é”URLã‚’å–å¾—
    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 2ç§’ï¼ˆv5.2-liteã§çŸ­ç¸®ï¼‰
    """
    try:
        response = requests.head(url, allow_redirects=True, timeout=timeout)
        final_url = clean_url(response.url)
        return final_url
    except requests.Timeout:
        STATS['redirect_timeout'] += 1
        return None
    except Exception as e:
        STATS['redirect_failed'] += 1
        return None

def extract_publisher_from_url(url):
    """URLã‹ã‚‰å‡ºå…¸ãƒ¡ãƒ‡ã‚£ã‚¢åã‚’æŠ½å‡º"""
    domain_mapping = {
        'cnyes.com': 'é‰…äº¨ç¶²',
        'ctee.com.tw': 'å·¥å•†æ™‚å ±',
        'technews.tw': 'TechNews ç§‘æŠ€æ–°å ±',
        'udn.com': 'è¯åˆæ–°èç¶²',
        'ltn.com.tw': 'è‡ªç”±æ™‚å ±',
        'chinatimes.com': 'ä¸­æ™‚æ–°èç¶²',
        'cna.com.tw': 'ä¸­å¤®ç¤¾ CNA',
        'moneydj.com': 'MoneyDJ',
        'eettaiwan.com': 'EE Times Taiwan',
        'digitimes.com.tw': 'DIGITIMES',
        'storm.mg': 'é¢¨å‚³åª’',
        'businessweekly.com.tw': 'å•†æ¥­å‘¨åˆŠ',
        'cw.com.tw': 'å¤©ä¸‹é›œèªŒ',
        'wealth.com.tw': 'è²¡è¨Š',
        'mirrormedia.mg': 'é¡é€±åˆŠ',
        'ettoday.net': 'ETtoday',
        'setn.com': 'ä¸‰ç«‹æ–°èç¶²',
        'nownews.com': 'NOWnews',
        'yahoo.com': 'Yahooå¥‡æ‘©',
        'pchome.com.tw': 'PChome',
        'cmoney.tw': 'CMoney',
        'moneysmart.tw': 'MoneySmart',
        'wealth.com.tw': 'è²¡è¨Š',
        'businesstoday.com.tw': 'ä»Šå‘¨åˆŠ',
        'smart.businessweekly.com.tw': 'Smartè‡ªå­¸ç¶²',
        'money-link.com.tw': 'ç†è²¡å‘¨åˆŠ',
        'moneyweekly.com.tw': 'ç†è²¡å‘¨åˆŠ',
        'ctee.com.tw': 'å·¥å•†æ™‚å ±',
        'economic.com.tw': 'ç¶“æ¿Ÿæ—¥å ±',
        'appledaily.com.tw': 'è˜‹æœæ–°èç¶²',
        'ctwant.com': 'CTWANT',
        'sinotrade.com.tw': 'æ°¸è±é‡‘è­‰åˆ¸',
        'wantgoo.com': 'ç©è‚¡ç¶²',
        'wantrich.chinatimes.com': 'æ—ºå¾—å¯Œç†è²¡ç¶²',
        'knowing.asia': 'knowing',
        'newtalk.tw': 'æ–°é ­æ®¼',
        'taiwannews.com.tw': 'Taiwan News',
        'rti.org.tw': 'ä¸­å¤®å»£æ’­é›»å°',
        'epochtimes.com': 'å¤§ç´€å…ƒ',
        'ntdtv.com': 'æ–°å”äºº',
        'voacantonese.com': 'ç¾åœ‹ä¹‹éŸ³',
        'rfi.fr': 'RFI',
        'bbc.com': 'BBC',
        'reuters.com': 'Reuters',
        'bloomberg.com': 'Bloomberg',
        'ft.com': 'Financial Times',
        'wsj.com': 'Wall Street Journal',
        'nikkei.com': 'æ—¥ç¶“ä¸­æ–‡ç¶²',
    }
    
    parsed = urlparse(url)
    domain = parsed.netloc.lower().replace('www.', '')
    
    for key, value in domain_mapping.items():
        if key in domain:
            return value
    
    return None

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆå…¨è§’/åŠè§’çµ±ä¸€ã€è¨˜å·é™¤å»ã€ç©ºç™½åœ§ç¸®ï¼‰"""
    # å…¨è§’â†’åŠè§’
    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
    # è¨˜å·é™¤å»
    text = re.sub(r'[^\w\s]', '', text)
    # ç©ºç™½åœ§ç¸®
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def generate_article_signature(title, publisher, pub_date, snippet):
    """è¨˜äº‹ç½²åã‚’ç”Ÿæˆï¼ˆé‡è¤‡æ’é™¤ç”¨ï¼‰"""
    normalized_title = normalize_text(title)
    normalized_snippet = normalize_text(snippet[:120])
    date_str = pub_date.strftime('%Y-%m-%d') if pub_date else 'unknown'
    
    signature_string = f"{normalized_title}|{publisher}|{date_str}|{normalized_snippet}"
    return hashlib.md5(signature_string.encode('utf-8')).hexdigest()

def load_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open('.taiwan_stock_news_cache_v5.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"news": {}, "topics": {}}

def save_cache(cache):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open('.taiwan_stock_news_cache_v5.json', 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def clean_cache(cache):
    """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    now = datetime.now(TW_TZ)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: 30æ—¥é–“ä¿æŒ
    if 'news' not in cache:
        cache['news'] = {}
    news_cutoff = (now - timedelta(days=30)).isoformat()
    cache['news'] = {sig: data for sig, data in cache['news'].items() 
                     if data.get('cached_at', '') > news_cutoff}
    
    # è«–ç‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: 7å–¶æ¥­æ—¥ï¼ˆç´„10æ—¥ï¼‰ä¿æŒ
    if 'topics' not in cache:
        cache['topics'] = {}
    topic_cutoff = (now - timedelta(days=10)).isoformat()
    cache['topics'] = {stock_id: data for stock_id, data in cache['topics'].items() 
                       if data.get('cached_at', '') > topic_cutoff}
    
    return cache

def process_rss_entry(entry, cache):
    """
    RSSã‚¨ãƒ³ãƒˆãƒªã‚’å‡¦ç†ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡ã‚’ã‚¹ã‚­ãƒƒãƒ—
    """
    rss_url = entry.get("link", "")
    title = entry.get("title", "")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆrss_urlãƒ™ãƒ¼ã‚¹ï¼‰
    if rss_url in cache.get('url_to_signature', {}):
        signature = cache['url_to_signature'][rss_url]
        if signature in cache['news']:
            STATS['cache_hit'] += 1
            cached_data = cache['news'][signature]
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰SNSåˆ¤å®š
            if is_sns_domain(cached_data['final_url']):
                STATS['sns_domain_excluded'] += 1
                return None
            return cached_data
    
    STATS['cache_miss'] += 1
    
    # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡
    final_url = resolve_final_url(rss_url, timeout=3)
    
    # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆæœªè§£æ±ºã¯é™¤å¤–
    if not final_url:
        return None
    
    # SNSãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
    if is_sns_domain(final_url):
        STATS['sns_domain_excluded'] += 1
        return None
    
    # å‡ºå…¸æŠ½å‡º
    publisher = None
    if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
        publisher = entry.source.title
    if not publisher:
        publisher = extract_publisher_from_url(final_url)
    
    # å‡ºå…¸ä¸æ˜ã¯é™¤å¤–
    if not publisher:
        STATS['unknown_publisher_excluded'] += 1
        return None
    
    # å‡ºå…¸ãŒSNSãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆã‚‚é™¤å¤–
    if any(sns in publisher.lower() for sns in ['facebook', 'twitter', 'x.com', 'instagram', 'line', 'threads']):
        STATS['sns_publisher_excluded'] += 1
        return None
    
    # æ—¥æ™‚å–å¾—
    pub_date = None
    if hasattr(entry, 'published'):
        try:
            pub_date = date_parser.parse(entry.published).astimezone(TW_TZ)
        except:
            pass
    
    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆå–å¾—
    snippet = entry.get("summary", "")[:200]
    
    # è¨˜äº‹ç½²åç”Ÿæˆ
    signature = generate_article_signature(title, publisher, pub_date, snippet)
    
    news_entry = {
        "title": title,
        "rss_url": rss_url,
        "final_url": final_url,
        "link": final_url,
        "publisher": publisher,
        "published": pub_date.isoformat() if pub_date else None,
        "snippet": snippet,
        "signature": signature,
        "cached_at": datetime.now(TW_TZ).isoformat()
    }
    
    return news_entry

def collect_news_parallel():
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¸¦åˆ—åé›†
    v5.2-lite-v2: éŠ˜æŸ„åˆ¥ã«URLè§£æ±ºï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ä¸Šä½20ä»¶ï¼‰ã€max_workers=10
    """
    print("ğŸ“° RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...", flush=True)
    cache = load_cache()
    
    # url_to_signatureãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åˆæœŸåŒ–
    if 'url_to_signature' not in cache:
        cache['url_to_signature'] = {}
    
    # v5.2-lite-v2: éŠ˜æŸ„åˆ¥ã«RSSåé›†ï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ä¸Šä½20ä»¶ï¼‰
    MAX_ENTRIES_PER_FEED = 20
    all_entries = []
    
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ä¸Šä½20ä»¶ã‚’å–å¾—
            all_entries.extend(feed.entries[:MAX_ENTRIES_PER_FEED])
        except Exception as e:
            print(f"âš ï¸  RSSåé›†ã‚¨ãƒ©ãƒ¼: {feed_url} - {e}", flush=True)
    
    print(f"  RSSåé›†å®Œäº†: {len(all_entries)}ä»¶ï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ä¸Šä½{MAX_ENTRIES_PER_FEED}ä»¶ï¼‰", flush=True)
    
    entries_to_process = all_entries
    
    # ä¸¦åˆ—å‡¦ç†ã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡ï¼ˆmax_workers=10ï¼‰
    news_list = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(process_rss_entry, entry, cache): entry for entry in entries_to_process}
        
        for i, future in enumerate(as_completed(futures), 1):
            if i % 50 == 0:
                print(f"  å‡¦ç†ä¸­: {i}/{len(entries_to_process)}ä»¶", flush=True)
            
            try:
                result = future.result()
                if result:
                    news_list.append(result)
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    cache['news'][result['signature']] = result
                    cache['url_to_signature'][result['rss_url']] = result['signature']
            except Exception as e:
                pass
    
    # é‡è¤‡é™¤å¤–
    unique_news = {}
    for news in news_list:
        signature = news['signature']
        if signature in unique_news:
            STATS['duplicate_excluded'] += 1
        else:
            unique_news[signature] = news
    
    print(f"âœ… é‡è¤‡é™¤å¤–å¾Œ: {len(unique_news)}ä»¶", flush=True)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    cache = clean_cache(cache)
    save_cache(cache)
    
    return list(unique_news.values())

def translate_title(title):
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾ã®é‡‘èãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ç°¡æ½”ã§æ­£ç¢ºãªç¿»è¨³ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": f"ä»¥ä¸‹ã®å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„:\n\n{title}"}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return title

def judge_relevance(stock_id, stock_name, news_list, max_items=15):
    """LLMã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é–¢é€£æ€§ã‚’åˆ¤å®šï¼ˆv5.2-lite-v3: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ€å¤§15ä»¶ï¼‰"""
    news_text = "\n\n".join([
        f"[{i+1}] {news['title']}\nå‡ºå…¸: {news['publisher']}\næ¦‚è¦: {news['snippet']}"
        for i, news in enumerate(news_list[:max_items])
    ])
    
    prompt = f"""
ã‚ãªãŸã¯å°æ¹¾æ ªã®æŠ•è³‡åˆ¤æ–­ã‚’æ”¯æ´ã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚

éŠ˜æŸ„: {stock_name}ï¼ˆ{stock_id}ï¼‰
æ¥­æ…‹: {STOCKS[stock_id]['business_type']}

ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‹ã‚‰ã€ã“ã®éŠ˜æŸ„ã®æŠ•è³‡åˆ¤æ–­ã«æœ‰åŠ¹ãªæƒ…å ±ã‚’å«ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸åˆ¥ã—ã¦ãã ã•ã„ã€‚

ã€åˆ¤å®šåŸºæº–ã€‘
- é–¢é€£ã‚ã‚Š: æ¥­ç¸¾ã€å—æ³¨ã€æŠ€è¡“ã€å¸‚å ´å‹•å‘ãªã©æŠ•è³‡åˆ¤æ–­ã«ç›´æ¥å½±éŸ¿ã™ã‚‹æƒ…å ±
- é–¢é€£æ€§ä¸æ˜: æ¥­ç•Œå…¨èˆ¬ã®è©±é¡Œã§ã€éŠ˜æŸ„ã¸ã®å½±éŸ¿ãŒä¸æ˜ç¢º
- å‚è€ƒå¤–: ç„¡é–¢ä¿‚ã€ã¾ãŸã¯æŠ•è³‡åˆ¤æ–­ã«ç„¡ä¾¡å€¤

ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ:
{news_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®å½¢å¼ã§JSONé…åˆ—ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„:
[
  {{"index": 1, "relevance": "é–¢é€£ã‚ã‚Š", "score": 85, "reason": "ç†ç”±"}},
  {{"index": 2, "relevance": "å‚è€ƒå¤–", "score": 20, "reason": "ç†ç”±"}}
]
"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾æ ªã®æŠ•è³‡åˆ¤æ–­ã‚’æ”¯æ´ã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        result_text = response.choices[0].message.content.strip()
        # JSONã‚’æŠ½å‡º
        json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
        if json_match:
            judgments = json.loads(json_match.group())
            return judgments
        return []
    except Exception as e:
        print(f"âš ï¸  é–¢é€£æ€§åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}", flush=True)
        return []

def generate_topic(stock_id, stock_name, relevant_news):
    """è«–ç‚¹ã‚’ç”Ÿæˆ"""
    cache = load_cache()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if stock_id in cache.get('topics', {}):
        cached_topic = cache['topics'][stock_id]
        cached_at = datetime.fromisoformat(cached_topic['cached_at'])
        if datetime.now(TW_TZ) - cached_at < timedelta(days=10):
            return cached_topic['topic']
    
    news_text = "\n\n".join([
        f"[{i+1}] {news['title']}\nå‡ºå…¸: {news['publisher']}\næ¦‚è¦: {news['snippet']}"
        for i, news in enumerate(relevant_news[:5])
    ])
    
    prompt = f"""
éŠ˜æŸ„: {stock_name}ï¼ˆ{stock_id}ï¼‰
æ¥­æ…‹: {STOCKS[stock_id]['business_type']}

ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ã€æŠ•è³‡å®¶ãŒã€Œä»Šå¾Œã©ã“ã‚’è¦‹ã‚‹ã¹ãã‹ã€ã¨ã„ã†è«–ç‚¹ã‚’1æ–‡ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ‹ãƒ¥ãƒ¼ã‚¹:
{news_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
è«–ç‚¹ã®ã¿ã‚’1æ–‡ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€Œ2ãƒŠãƒè£½é€ æŠ€è¡“ã®é‡ç”£é–‹å§‹æ™‚æœŸã¨CoWoSå—æ³¨å¢—åŠ ãŒåç›Šæ‹¡å¤§ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ãŒç„¦ç‚¹ã€ï¼‰
"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾æ ªã®æŠ•è³‡åˆ¤æ–­ã‚’æ”¯æ´ã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5
        )
        
        topic = response.choices[0].message.content.strip()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if 'topics' not in cache:
            cache['topics'] = {}
        cache['topics'][stock_id] = {
            'topic': topic,
            'cached_at': datetime.now(TW_TZ).isoformat()
        }
        save_cache(cache)
        
        return topic
    except Exception as e:
        print(f"âš ï¸  è«–ç‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", flush=True)
        return "å¸‚å ´å‹•å‘ã¨æ¥­ç¸¾ã¸ã®å½±éŸ¿ã‚’æ³¨è¦–"

def send_email(results, taipei_time):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡"""
    from email_template_v5 import generate_html_email
    
    html_content = generate_html_email(results, taipei_time)
    
    message = Mail(
        from_email='adamsbmb0825@gmail.com',
        to_emails='adamsbmb0825@gmail.com',
        subject=f'ğŸ‡¹ğŸ‡¼ å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ v5.0 - {taipei_time.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}',
        html_content=html_content
    )
    
    try:
        sg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))
        response = sg.send(message)
        print(f"âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡æˆåŠŸï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}ï¼‰", flush=True)
    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}", flush=True)

def print_stats():
    """çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›"""
    print("\n" + "="*60)
    print("ğŸ“Š çµ±è¨ˆæƒ…å ±")
    print("="*60)
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {STATS['cache_hit']}ä»¶", flush=True)
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: {STATS['cache_miss']}ä»¶", flush=True)
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {STATS['cache_hit'] / (STATS['cache_hit'] + STATS['cache_miss']) * 100:.1f}%" if (STATS['cache_hit'] + STATS['cache_miss']) > 0 else "N/A")
    print(f"ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {STATS['redirect_timeout']}ä»¶", flush=True)
    print(f"ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå¤±æ•—: {STATS['redirect_failed']}ä»¶", flush=True)
    print(f"SNSãƒ‰ãƒ¡ã‚¤ãƒ³é™¤å¤–: {STATS['sns_domain_excluded']}ä»¶", flush=True)
    print(f"SNSå‡ºå…¸é™¤å¤–: {STATS['sns_publisher_excluded']}ä»¶", flush=True)
    print(f"å‡ºå…¸ä¸æ˜é™¤å¤–: {STATS['unknown_publisher_excluded']}ä»¶", flush=True)
    print(f"é‡è¤‡é™¤å¤–: {STATS['duplicate_excluded']}ä»¶", flush=True)
    print("="*60 + "\n")

def main():
    import os
    
    print("="*60)
    print(f"å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  {VERSION}", flush=True)
    print("="*60)
    
    # ç¬¬1æ®µéš: ç›´è¿‘7æ—¥ãƒ¢ãƒ¼ãƒ‰
    print("\n=== ç¬¬1æ®µéš: ç›´è¿‘7æ—¥ãƒ¢ãƒ¼ãƒ‰ ===", flush=True)
    all_news_7days = collect_news_parallel()
    
    # çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
    print_stats()
    
    # å„éŠ˜æŸ„ã®å‡¦ç†ï¼ˆç¬¬1æ®µéšï¼‰
    results = {}
    stocks_need_fallback = []  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒå¿…è¦ãªéŠ˜æŸ„
    
    for stock_id, stock_info in STOCKS.items():
        print("="*60)
        print(f"ğŸ“Š {stock_info['name']}ï¼ˆ{stock_id}ï¼‰", flush=True)
        print("="*60)
        
        # é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
        stock_keywords = [stock_info['name'], stock_id]
        # å®‡ç»ã®å ´åˆã¯Apacerã‚‚è¿½åŠ 
        if stock_id == '8271':
            stock_keywords.append('Apacer')
            stock_keywords.append('apacer')
        candidate_news = [news for news in all_news_7days if any(kw in news['title'] or kw in news['snippet'] for kw in stock_keywords)]
        
        print(f"å€™è£œãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(candidate_news)}ä»¶", flush=True)
        
        if len(candidate_news) == 0:
            print("âš ï¸  é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œ)", flush=True)
            stocks_need_fallback.append((stock_id, stock_info))
            continue
        
        # v5.2-lite: LLMé–¢é€£æ€§åˆ¤å®šã¯ä¸Šä½15ä»¶ã¾ã§
        judgments = judge_relevance(stock_id, stock_info['name'], candidate_news)
        
        # é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
        relevant_news = []
        for judgment in judgments:
            if judgment['relevance'] == 'é–¢é€£ã‚ã‚Š':
                idx = judgment['index'] - 1
                if idx < len(candidate_news):
                    news = candidate_news[idx].copy()
                    news['relevance_score'] = judgment['score']
                    news['relevance_reason'] = judgment['reason']
                    # ã‚¿ã‚¤ãƒˆãƒ«ç¿»è¨³
                    print(f"  [ç¿»è¨³ä¸­] {news['title'][:50]}...", flush=True)
                    news['title_ja'] = translate_title(news['title'])
                    relevant_news.append(news)
        
        print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(relevant_news)}ä»¶", flush=True)
        
        if len(relevant_news) == 0:
            print("âš ï¸  é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œ)", flush=True)
            stocks_need_fallback.append((stock_id, stock_info))
            continue
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        relevant_news.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # è«–ç‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        clustering_result = cluster_news_by_topic(stock_info['name'], relevant_news)
        print_clustering_log(stock_info['name'], clustering_result)
        
        # é…ä¿¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æº–å‚™ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ä»˜ãï¼‰
        delivery_news = prepare_delivery_news(clustering_result, max_clusters=3)
        
        print(f"âœ… é…ä¿¡: {len(delivery_news)}ã‚¯ãƒ©ã‚¹ã‚¿", flush=True)
        
        # è«–ç‚¹ç”Ÿæˆ
        topic = generate_topic(stock_id, stock_info['name'], relevant_news)
        
        results[stock_id] = {
            'stock_info': stock_info,
            'topic': topic,
            'news': delivery_news,
            'is_single_event': clustering_result['is_single_event'],
            'event_description': clustering_result['event_description']
        }
    
    # ç¬¬2æ®µéš: 30æ—¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸è¶³éŠ˜æŸ„ã®ã¿ï¼‰
    if stocks_need_fallback:
        print("\n=== ç¬¬2æ®µéš: 30æ—¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ===", flush=True)
        print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾è±¡: {len(stocks_need_fallback)}éŠ˜æŸ„", flush=True)
        
        # 30æ—¥åˆ†ã®RSSåé›†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        # TODO: å®Ÿè£…ã‚’ç°¡ç•¥åŒ–ã™ã‚‹ãŸã‚ã€7æ—¥åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é…ã‚Œã¦ã‚‚ä¾¡å€¤ãŒã‚ã‚‹é¡å‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        # å®Ÿéš›ã®30æ—¥åˆ†ã®åé›†ã¯æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å®Ÿè£…
        
        for stock_id, stock_info in stocks_need_fallback:
            print("="*60)
            print(f"ğŸ“Š {stock_info['name']}ï¼ˆ{stock_id}ï¼‰- 30æ—¥æ‹¡å¼µ", flush=True)
            print("="*60)
            
            # 7æ—¥åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é…ã‚Œã¦ã‚‚ä¾¡å€¤ãŒã‚ã‚‹é¡å‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            stock_keywords = [stock_info['name'], stock_id]
            if stock_id == '8271':
                stock_keywords.append('Apacer')
                stock_keywords.append('apacer')
            
            # é…ã‚Œã¦ã‚‚ä¾¡å€¤ãŒã‚ã‚‹é¡å‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’æŠ½å‡º
            candidate_news_30days = [
                news for news in all_news_7days 
                if any(kw in news['title'] or kw in news['snippet'] for kw in stock_keywords)
                and is_delayed_valuable_news(news['title'], news['snippet'])
            ]
            
            print(f"30æ—¥æ‹¡å¼µå€™è£œãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(candidate_news_30days)}ä»¶", flush=True)
            
            if len(candidate_news_30days) == 0:
                print("âš ï¸  30æ—¥æ‹¡å¼µã§ã‚‚é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—", flush=True)
                continue
            
            # LLMé–¢é€£æ€§åˆ¤å®šï¼ˆæœ€å¤§10ä»¶ï¼‰
            judgments_30days = judge_relevance(stock_id, stock_info['name'], candidate_news_30days, max_items=10)
            
            # é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
            relevant_news_30days = []
            for judgment in judgments_30days:
                if judgment['relevance'] == 'é–¢é€£ã‚ã‚Š':
                    idx = judgment['index'] - 1
                    if idx < len(candidate_news_30days):
                        news = candidate_news_30days[idx].copy()
                        news['relevance_score'] = judgment['score']
                        news['relevance_reason'] = judgment['reason']
                        # ã‚¿ã‚¤ãƒˆãƒ«ç¿»è¨³
                        print(f"  [ç¿»è¨³ä¸­] {news['title'][:50]}...", flush=True)
                        news['title_ja'] = translate_title(news['title'])
                        relevant_news_30days.append(news)
            
            print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(relevant_news_30days)}ä»¶", flush=True)
            
            if len(relevant_news_30days) == 0:
                print("âš ï¸  30æ—¥æ‹¡å¼µã§ã‚‚é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—", flush=True)
                continue
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            relevant_news_30days.sort(key=lambda x: x['relevance_score'], reverse=True)
            
            # è«–ç‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            clustering_result = cluster_news_by_topic(stock_info['name'], relevant_news_30days)
            print_clustering_log(stock_info['name'], clustering_result)
            
            # é…ä¿¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æº–å‚™ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ä»˜ãï¼‰
            delivery_news = prepare_delivery_news(clustering_result, max_clusters=3)
            
            print(f"âœ… é…ä¿¡: {len(delivery_news)}ã‚¯ãƒ©ã‚¹ã‚¿", flush=True)
            
            # è«–ç‚¹ç”Ÿæˆ
            topic = generate_topic(stock_id, stock_info['name'], relevant_news_30days)
            
            results[stock_id] = {
                'stock_info': stock_info,
                'topic': topic,
                'news': delivery_news,
                'is_single_event': clustering_result['is_single_event'],
                'event_description': clustering_result['event_description']
            }
    
    # æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼
    print("\n=== æœ€çµ‚çµæœ ===", flush=True)
    for stock_id, result in results.items():
        stock_name = result['stock_info']['name']
        news_count = len(result['news'])
        print(f"{stock_name}ï¼ˆ{stock_id}ï¼‰: é…ä¿¡{news_count}ã‚¯ãƒ©ã‚¹ã‚¿", flush=True)
    
    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡
    if results:
        now_taipei = datetime.now(TW_TZ)
        print("\nğŸ“§ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ä¸­...", flush=True)
        send_email(results, now_taipei)
    else:
        print("\nâš ï¸  é…ä¿¡ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“", flush=True)

if __name__ == "__main__":
    main()
